* decision boundary given by SVM, Fisher LDA, logistic regression.
* generalized eigenvalue/eigenvector
* 2nd derivative to find maxima
* how to choose $\gamma$.
  * do we not need any distance metric in the error function?
* want-
  * min $\sigma_1^{_2}$ and $\sigma_2^{_2}$
    * same as min $\sigma_1^{_2}+\sigma_1^{_2}$
    * same as max $(1/\sigma_1^{_2}+\sigma_1^{_2})$
* Bayes decision theory.  
  * Chapter 2 Duda and Hart
  * Eq 13 imp
  * Eq 17. Special case of 2 classes
  * Eq 19. "The risk corresponding to this loss function is precisely the average probability of error, since the conditional risk is"- Good.
  * Eq 20. "to minimize the average probability of error, we should select the i that maximizes the posterior probability"

  ![](yourscanfromsnelllibrary/image0000.jpg)
  ![](yourscanfromsnelllibrary/image0001.jpg)
  ![](yourscanfromsnelllibrary/image0002.jpg)
  ![](yourscanfromsnelllibrary/image0003.jpg)
  ![](yourscanfromsnelllibrary/image0004.jpg)
  ![](yourscanfromsnelllibrary/image0005.jpg)
  ![](yourscanfromsnelllibrary/image0006.jpg)
  ![](yourscanfromsnelllibrary/image0007.jpg)
  ![](yourscanfromsnelllibrary/image0008.jpg)
  ![](yourscanfromsnelllibrary/image0009.jpg)
  ![](yourscanfromsnelllibrary/image0010.jpg)
  ![](yourscanfromsnelllibrary/image0011.jpg)
